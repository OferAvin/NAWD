{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import mne\n",
    "import sklearn\n",
    "import os \n",
    "import time\n",
    "import random\n",
    "import time\n",
    "import scipy.linalg\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from itertools import chain, product\n",
    "import pickle # to write results to file\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.module import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy.stats import norm, wasserstein_distance\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess whether GPU is availble\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using the GPU.\")\n",
    "    print(\"Device name - \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else: \n",
    "    print(\"PyTorch is not using the GPU.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa436bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Utility functions frmo diffrent notebooks\n",
    "import import_ipynb\n",
    "from IEEE_data import extract_ieee_data, LazyProperty, data_4class\n",
    "from CHIST_ERA_data import *\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77144ecd",
   "metadata": {},
   "source": [
    "### Datset and Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_AE(LightningModule):\n",
    "    def __init__(self, input_channels, days_labels_N, task_labels_N, learning_rate=1e-3, filters_n = [32, 16, 4], mode = 'supervised'):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.filters_n = filters_n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        self.l1_filters, self.l2_filters, self.l3_filters = self.filters_n\n",
    "        self.mode = mode\n",
    "        self.switcher = True\n",
    "        ### The model architecture ###\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=25, stride=5, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "#         nn.MaxPool1d(kernel_size=15, stride=3),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "        # IMPORTENT - on the IEEE dataset - the output padding needs to be 1 in the row below -on CHIST-ERA its 1\n",
    "        nn.ConvTranspose1d(self.l3_filters, self.l2_filters, kernel_size=5, stride=2, padding=1, output_padding=0),\n",
    "#         nn.Dropout1d(p=0.33),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='linear'),\n",
    "        nn.ConvTranspose1d(self.l2_filters, self.l1_filters, kernel_size=10, stride=2, padding=1, output_padding=0),\n",
    "#         nn.Dropout1d(p=0.33),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='linear'),\n",
    "        nn.ConvTranspose1d(self.l1_filters, self.input_channels, kernel_size=25, stride=5, padding=1, output_padding=2),\n",
    "        )\n",
    "        \n",
    "        # Residuals Encoder\n",
    "        self.res_encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=25, stride=5, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Classifier Days\n",
    "        self.classiffier_days = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4704, days_labels_N),\n",
    "        nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Classifier Task\n",
    "        self.classiffier_task = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4704, task_labels_N),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        )\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch > 200:\n",
    "            self.unfreeze_decoder()\n",
    "            self.unfreeze_encoder()\n",
    "            self.mode = 'all'\n",
    "    \n",
    "        if self.current_epoch % 20 == 0:\n",
    "            self.switcher = not self.switcher\n",
    "            if self.switcher == True:\n",
    "                self.freeze_decoder()\n",
    "                self.unfreeze_encoder()\n",
    "                self.mode = 'task'\n",
    "            elif self.switcher == False:\n",
    "                self.freeze_encoder()\n",
    "                self.unfreeze_decoder()\n",
    "                self.mode = 'reconstruction'\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract batch\n",
    "        x, y, days_y = batch\n",
    "        # Define loss functions\n",
    "        loss_fn_days = nn.CrossEntropyLoss()\n",
    "        loss_fn_rec = nn.MSELoss()\n",
    "        loss_fn_task = nn.CrossEntropyLoss()\n",
    "            \n",
    "        # Encode\n",
    "        encoded = self.encode(x)\n",
    "        \n",
    "        # Get predictions for task\n",
    "        preds_task = self.classiffier_task(encoded)\n",
    "        task_loss = loss_fn_task(preds_task, y)\n",
    "\n",
    "        # Compute task classification accuracy\n",
    "        task_acc = sklearn.metrics.accuracy_score(np.argmax(F.softmax(preds_task, dim=-1).detach().cpu().numpy(), axis=1),\n",
    "                                             np.argmax(y.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "        # Log scalars\n",
    "        self.log('task_loss', task_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('task_accuracy', task_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(encoded)\n",
    "\n",
    "        # Compute residuals\n",
    "        residuals = torch.sub(x, reconstructed)\n",
    "\n",
    "        # Encode residuals\n",
    "        residuals_compact = self.res_encoder(residuals)\n",
    "\n",
    "        # Get predictions per day\n",
    "        preds_days = self.classiffier_days(residuals_compact)\n",
    "\n",
    "        # Compute all losses\n",
    "        days_loss = loss_fn_days(preds_days, days_y)\n",
    "        reconstruction_loss = loss_fn_rec(reconstructed, x)\n",
    "\n",
    "        # Compute days classification accuracy\n",
    "        days_acc = sklearn.metrics.accuracy_score(np.argmax(F.softmax(preds_days, dim=-1).detach().cpu().numpy(), axis=1),\n",
    "                                             np.argmax(days_y.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "        # Log results\n",
    "        self.log('days_loss', days_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('reconstruction_loss', reconstruction_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('days_accuracy', days_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        if self.mode == 'task':\n",
    "            return days_loss + task_loss\n",
    "        elif self.mode == 'reconstruction':\n",
    "            return reconstruction_loss\n",
    "        elif self.mode == 'all':\n",
    "            return reconstruction_loss + days_loss + task_loss\n",
    "   \n",
    "    def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "    \n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_encoder(self):\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def freeze_decoder(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_decoder(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "            \n",
    "    def change_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d26a62",
   "metadata": {},
   "source": [
    "## Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_days, dictListStacked, ae_learning_rt, convolution_filters, batch_sz, epoch_n, proccessor):\n",
    "    \n",
    "    # check if enough train days exists\n",
    "    if train_days[1] >= len(dictListStacked):\n",
    "        raise Exception(\"Not enough training days\")\n",
    "\n",
    "\n",
    "    # device settings\n",
    "    device = torch.device(proccessor)\n",
    "    accelerator = proccessor if proccessor=='cpu' else 'gpu' \n",
    "    devices = 1 if proccessor=='cpu' else -1 \n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger('../tb_logs', name='EEG_Logger')\n",
    "    # Shuffle the days\n",
    "    random.shuffle(dictListStacked)\n",
    "    # Train Dataset\n",
    "    signal_data = EEGDataSet_signal_by_day(dictListStacked, train_days)\n",
    "    signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "    x, y, days_y = signal_data.getAllItems()\n",
    "    y = np.argmax(y, -1)\n",
    "    days_labels_N = signal_data.days_labels_N\n",
    "    task_labels_N = signal_data.task_labels_N\n",
    "\n",
    "    # Train model on training day\n",
    "    metrics = ['classification_loss', 'reconstruction_loss']\n",
    "    day_zero_AE = convolution_AE(signal_data.n_channels, days_labels_N, task_labels_N, ae_learning_rt, filters_n=convolution_filters, mode='supervised')\n",
    "    day_zero_AE.to(device)\n",
    "\n",
    "    trainer_2 = pl.Trainer(max_epochs=epoch_n, logger=logger, accelerator=accelerator , devices=devices)\n",
    "    trainer_2.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "    \n",
    "    # CV On the training set (with and without ae)\n",
    "    ws_ae_train, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N=5, classifier=False)\n",
    "    ws_train, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N=5, classifier=False)\n",
    "    \n",
    "\n",
    "    test_days = [train_days[1], len(dictListStacked)]\n",
    "\n",
    "    # Create test Datasets\n",
    "    signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "\n",
    "    # get data\n",
    "    signal_test, y_test = signal_test_data.getAllItems()\n",
    "    # reconstruct EEG using day 0 AE\n",
    "    rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "    # Use models\n",
    "    # within session cv on the test set (mean on test set)\n",
    "    ws_test, _ = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N=5, classifier = False)\n",
    "    # Using day 0 classifier for test set inference (mean on test set)\n",
    "    bs_test = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N=5, classifier=day_zero_bench_clf)\n",
    "    # Using day 0 classifier + AE for test set inference (mean on test set)\n",
    "    bs_ae_test = csp_score(rec_signal_zero, y_test, cv_N=5, classifier=day_zero_AE_clf)\n",
    "    \n",
    "    return ws_train, ws_ae_train, ws_test, bs_test, bs_ae_test, day_zero_AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2af1c",
   "metadata": {},
   "source": [
    "#### Load the files - IEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a213a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subs_EEG_dict(props):\n",
    "    all_sub_EEG_dict = {}\n",
    "    for sub in props['sub_list']:  \n",
    "        try:\n",
    "            dictListStacked = extract_ieee_data(\n",
    "                sub, props['filterLim'],props['tmin'], props['tmax'], props['select_label'], props['data_dir'])\n",
    "        except:\n",
    "            print('Could\\'nt load data files')\n",
    "        # Remove noisy trials using amplitude threshold\n",
    "        new_dict_list = []\n",
    "        for i, D in enumerate(dictListStacked):\n",
    "            max_amp = np.amax(np.amax(D['segmentedEEG'], 2), 1)\n",
    "            min_amp = np.amin(np.amin(D['segmentedEEG'], 2), 1)\n",
    "            max_tr = max_amp > props['amp_thresh']\n",
    "            min_tr = min_amp < -props['amp_thresh']\n",
    "            noisy_trials = [a or b for a, b in zip(max_tr, min_tr)]\n",
    "            D['segmentedEEG'] = np.delete(D['segmentedEEG'], noisy_trials,axis=0)\n",
    "            D['labels'] = np.delete(D['labels'], noisy_trials,axis=0)\n",
    "            if D['segmentedEEG'].shape[0] > props['min_trials']:\n",
    "                    new_dict_list.append(D)\n",
    "\n",
    "        all_sub_EEG_dict[sub] = new_dict_list\n",
    "    \n",
    "    return all_sub_EEG_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "props_dict = {\n",
    "    'device' : 'cuda', # 'cpu' 'cuda'\n",
    "    \n",
    "    'tmin' : 0,\n",
    "    'tmax' : 6,\n",
    "    'select_label' : [1, 4],\n",
    "\n",
    "    'filterLim' : [1,40], # In Hz\n",
    "\n",
    "    'fs' : 100,\n",
    "    'ae_lrn_rt' : 3e-4,\n",
    "    'n_epochs' : 250,\n",
    "    'btch_sz' : 8,\n",
    "    'cnvl_filters' : [8, 16, 32],\n",
    "\n",
    "    'n_feature_select' : '250',\n",
    "\n",
    "    'amp_thresh' : 250,\n",
    "    'min_trials' : 10,\n",
    "\n",
    "    'sub_list' : ['A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8','S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8','S9','S10','S11', 'S12'],\n",
    "    \n",
    "    'data_dir' : '../data/ieee_dataset/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs/EEG_Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_subs_multi_iterations(props, subs_EEG_dict, train_days_range = [1,7], iterations_per_day = 250):\n",
    "    \n",
    "# This function runs multi iterations experiment over all subjects.\n",
    "# The experiment runs all ranges of traning days from 0-{train_days_range[0]} to 0-{train_days_range[1]}.\n",
    "# Every iteration models are trained for all ranges of training days and all subjects.\n",
    "# the function saves 2 dictionaries to 2 files:\n",
    "    # task clasification results dictionary\n",
    "    # origin day clasification results dictionary\n",
    "# The function returns the 2 file pathes\n",
    "    \n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    task_iter_dict = {} # keys: iterations, vals: dict of dicts of dicts of scores for each sub\n",
    "    origin_iter_dict = {} \n",
    "    \n",
    "    for itr in range(iterations_per_day):\n",
    "        task_days_range_dict = {} # keys: train days range, vals: dict of dicts of scores for each sub\n",
    "        origin_days_range_dict = {}\n",
    "        \n",
    "        for last_train_day in range(train_days_range[0],train_days_range[1]):\n",
    "            task_sub_dict = {} # keys: sub, vals: dict of list of the scores dicts for each sub\n",
    "            origin_sub_dict = {}\n",
    "            \n",
    "            curr_days_rng=[0, last_train_day] # determine the current range for training days \n",
    "            rng_str = '-'.join(str(e) for e in curr_days_rng) # turn days range list to str to use as key name\n",
    "                  \n",
    "            for sub in list(subs_EEG_dict.keys()):\n",
    "                print(f'\\niter: {itr}, last training day: {last_train_day}, sub: {sub}...')\n",
    "                \n",
    "                task_per_sub_scores_dict = {} # keys: method(ws,bs,AE), vals: scores\n",
    "                origin_per_sub_scores_dict = {} # keys: signal(orig,rec,res), vals: scores\n",
    "                  \n",
    "                print('training model...')\n",
    "                try:\n",
    "                    ws_train, ws_ae_train, ws_test, bs_test, ae_test, day_zero_AE = \\\n",
    "                    training_loop(curr_days_rng, subs_EEG_dict[sub], props['ae_lrn_rt'], \\\n",
    "                                props['cnvl_filters'], props['btch_sz'], props['n_epochs'], props['device'])\n",
    "                except Exception as e:\n",
    "                    print(f'Can\\'t train a model for sub: {sub} with last training day: {last_train_day} because:')\n",
    "                    print(e)\n",
    "                    continue\n",
    "                \n",
    "                # Add task classification results\n",
    "                task_per_sub_scores_dict['ws_train'] = ws_train\n",
    "                task_per_sub_scores_dict['ae_train'] = ws_ae_train\n",
    "                task_per_sub_scores_dict['ws_test'] = ws_test\n",
    "                task_per_sub_scores_dict['bs_test'] = bs_test\n",
    "                task_per_sub_scores_dict['ae_test'] = ae_test\n",
    "                \n",
    "                # Day classfication using residuals original and recontrusted EEG\n",
    "                print('classifying origin day...')\n",
    "                orig_score, rec_score, res_score = origin_day_clf(subs_EEG_dict[sub], day_zero_AE)\n",
    "                origin_per_sub_scores_dict['orig'] = orig_score\n",
    "                origin_per_sub_scores_dict['rec'] = rec_score\n",
    "                origin_per_sub_scores_dict['res'] = res_score\n",
    "            \n",
    "                task_sub_dict[sub] = task_per_sub_scores_dict\n",
    "                origin_sub_dict[sub] = origin_per_sub_scores_dict\n",
    "\n",
    "            \n",
    "            task_days_range_dict[rng_str] = task_sub_dict\n",
    "            origin_days_range_dict[rng_str] = origin_sub_dict\n",
    "                   \n",
    "        task_iter_dict[itr] = task_days_range_dict\n",
    "        origin_iter_dict[itr] = origin_days_range_dict\n",
    "        \n",
    "        # save to file\n",
    "        print('save to file...')\n",
    "        f_task_path = f'../results/task_iters_timestr_{ts}.pickle'\n",
    "        f_origin_path = f'../results/origin_iters_timestr_{ts}.pickle'\n",
    "        \n",
    "        try:\n",
    "            f_task = open(f_task_path, 'wb')\n",
    "            f_origin = open(f_origin_path, 'wb')\n",
    "            pickle.dump(task_iter_dict, f_task)\n",
    "            pickle.dump(origin_iter_dict, f_origin)\n",
    "        except:\n",
    "            print(\"Couldn't save to file\")\n",
    "        finally:\n",
    "            f_task.close()\n",
    "            f_origin.close()\n",
    "        \n",
    "        print(f'stopped after {itr+1} iterations')\n",
    "    return f_task_path, f_origin_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_result_from_file(f_name):\n",
    "    # extract the data from {f_name} and calculates the mean for exch method over iterations and subjects\n",
    "    # possible methods,\n",
    "        # for task: ws_test, bs_test, ae_test, ws_train, ae_train\n",
    "        # for origin day: orig, rec, res\n",
    "    #return\n",
    "        # {mtd_by_rng_result_dict} mean result for each range by mthod\n",
    "        # {all_train_ranges} list of all ranges\n",
    "        # {methods} list of all methods\n",
    "        \n",
    "    with open(f_name, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    all_iters = list(results_dict.keys())\n",
    "    all_train_ranges = list(results_dict[all_iters[0]].keys())\n",
    "    sub_list = list(results_dict[all_iters[0]][all_train_ranges[0]].keys())\n",
    "    methods = list(results_dict[all_iters[0]][all_train_ranges[0]][sub_list[0]].keys())\n",
    "\n",
    "    all_rng_result_dict = {}\n",
    "    \n",
    "    for rng in all_train_ranges:\n",
    "        mtd_result_dict = {}\n",
    "        range_subs = list(results_dict[0][rng].keys()) # range might not contains all subs\n",
    "\n",
    "        for mtd in methods:\n",
    "            # collect all results for rng and mtd (from all iters and subs)\n",
    "            result_per_rng_and_mtd = [results_dict[itr][rng][sub][mtd] for itr in all_iters for sub in range_subs]\n",
    "            mtd_result_dict[mtd] = np.mean(result_per_rng_and_mtd)\n",
    "        \n",
    "        all_rng_result_dict[rng] = mtd_result_dict\n",
    "    return all_rng_result_dict, all_train_ranges, methods, len(all_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec05ac1",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data for subs \n",
    "all_sub_EEG_dict = get_all_subs_EEG_dict(props_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edf254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multi experimant\n",
    "f_task_name, f_res_name = run_all_subs_multi_iterations(props_dict, all_sub_EEG_dict, [1,2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceebdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean results over iterations and subjects for task and origin day classification\n",
    "task_result, rng_list, task_mtd_list, n_itr = get_mean_result_from_file(f_task_name)\n",
    "origin_result, rng_list, mtd_list = get_mean_result_from_file(f_res_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2c09c",
   "metadata": {},
   "source": [
    "## End Of Experiment!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "35d426c32d3c3d4800097806a9597474a84165f20bd11beece54e5b9a2bb14ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
