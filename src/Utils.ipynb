{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import scipy\n",
    "import torch\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.io import savemat\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSP:\n",
    "    def __init__(self,m_filters):\n",
    "        self.m_filters = m_filters\n",
    "\n",
    "    def fit(self,x_train,y_train):\n",
    "        x_data = np.copy(x_train)\n",
    "        y_labels = np.copy(y_train)\n",
    "        n_trials, n_channels, n_samples = x_data.shape\n",
    "        cov_x = np.zeros((2, n_channels, n_channels), dtype=np.float64)\n",
    "        for i in range(n_trials):\n",
    "            x_trial = x_data[i, :, :]\n",
    "            y_trial = y_labels[i]\n",
    "            cov_x_trial = np.matmul(x_trial, np.transpose(x_trial))\n",
    "            cov_x_trial /= np.trace(cov_x_trial)\n",
    "            cov_x[y_trial, :, :] += cov_x_trial\n",
    "\n",
    "        cov_x = np.asarray([cov_x[cls]/np.sum(y_labels==cls) for cls in range(2)])\n",
    "        cov_combined = cov_x[0]+cov_x[1]\n",
    "        eig_values, u_mat = scipy.linalg.eig(cov_combined,cov_x[0])\n",
    "        sort_indices = np.argsort(abs(eig_values))[::-1]\n",
    "        eig_values = eig_values[sort_indices]\n",
    "        u_mat = u_mat[:,sort_indices]\n",
    "        u_mat = np.transpose(u_mat)\n",
    "\n",
    "        return eig_values, u_mat\n",
    "\n",
    "    def transform(self,x_trial,eig_vectors):\n",
    "        z_trial = np.matmul(eig_vectors, x_trial)\n",
    "        z_trial_selected = z_trial[:self.m_filters,:]\n",
    "        z_trial_selected = np.append(z_trial_selected,z_trial[-self.m_filters:,:],axis=0)\n",
    "        sum_z2 = np.sum(z_trial_selected**2, axis=1)\n",
    "        sum_z = np.sum(z_trial_selected, axis=1)\n",
    "        var_z = (sum_z2 - (sum_z ** 2)/z_trial_selected.shape[1]) / (z_trial_selected.shape[1] - 1)\n",
    "        sum_var_z = sum(var_z)\n",
    "        return np.log(var_z/sum_var_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bac3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBCSP:\n",
    "    def __init__(self,m_filters):\n",
    "        self.m_filters = m_filters\n",
    "        self.fbcsp_filters_multi=[]\n",
    "\n",
    "    def fit(self,x_train_fb,y_train):\n",
    "        y_classes_unique = np.unique(y_train)\n",
    "        n_classes = len(y_classes_unique)\n",
    "        self.csp = CSP(self.m_filters)\n",
    "\n",
    "        def get_csp(x_train_fb, y_train_cls):\n",
    "            fbcsp_filters = {}\n",
    "            for j in range(x_train_fb.shape[0]):\n",
    "                x_train = x_train_fb[j, :, :, :]\n",
    "                eig_values, u_mat = self.csp.fit(x_train, y_train_cls)\n",
    "                fbcsp_filters.update({j: {'eig_val': eig_values, 'u_mat': u_mat}})\n",
    "            return fbcsp_filters\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            cls_of_interest = y_classes_unique[i]\n",
    "            select_class_labels = lambda cls, y_labels: [0 if y == cls else 1 for y in y_labels]\n",
    "            y_train_cls = np.asarray(select_class_labels(cls_of_interest, y_train))\n",
    "            fbcsp_filters=get_csp(x_train_fb,y_train_cls)\n",
    "            self.fbcsp_filters_multi.append(fbcsp_filters)\n",
    "\n",
    "    def transform(self,x_data,class_idx=0):\n",
    "        n_fbanks, n_trials, n_channels, n_samples = x_data.shape\n",
    "        x_features = np.zeros((n_trials,self.m_filters*2*len(x_data)),dtype=np.float64)\n",
    "        for i in range(n_fbanks):\n",
    "            eig_vectors = self.fbcsp_filters_multi[class_idx].get(i).get('u_mat')\n",
    "            eig_values = self.fbcsp_filters_multi[class_idx].get(i).get('eig_val')\n",
    "            for k in range(n_trials):\n",
    "                x_trial = np.copy(x_data[i,k,:,:])\n",
    "                csp_feat = self.csp.transform(x_trial,eig_vectors)\n",
    "                for j in range(self.m_filters):\n",
    "                    x_features[k, i * self.m_filters * 2 + (j+1) * 2 - 2]  = csp_feat[j]\n",
    "                    x_features[k, i * self.m_filters * 2 + (j+1) * 2 - 1]= csp_feat[-j-1]\n",
    "\n",
    "        return x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelect:\n",
    "    def __init__(self, n_features_select=4, n_csp_pairs=2):\n",
    "        self.n_features_select = n_features_select\n",
    "        self.n_csp_pairs = n_csp_pairs\n",
    "        self.features_selected_indices=[]\n",
    "\n",
    "    def fit(self,x_train_features,y_train):\n",
    "        MI_features = self.MIBIF(x_train_features, y_train)\n",
    "        MI_sorted_idx = np.argsort(MI_features)[::-1]\n",
    "        features_selected = MI_sorted_idx[:self.n_features_select]\n",
    "\n",
    "        paired_features_idx = self.select_CSP_pairs(features_selected, self.n_csp_pairs)\n",
    "        x_train_features_selected = x_train_features[:, paired_features_idx]\n",
    "        self.features_selected_indices = paired_features_idx\n",
    "\n",
    "        return x_train_features_selected\n",
    "\n",
    "    def transform(self,x_test_features):\n",
    "        return x_test_features[:,self.features_selected_indices]\n",
    "\n",
    "    def MIBIF(self, x_features, y_labels):\n",
    "        def get_prob_pw(x,d,i,h):\n",
    "            n_data = d.shape[0]\n",
    "            t=d[:,i]\n",
    "            kernel = lambda u: np.exp(-0.5*(u**2))/np.sqrt(2*np.pi)\n",
    "            prob_x = 1 / (n_data * h) * sum(kernel((np.ones((len(t)))*x- t)/h))\n",
    "            return prob_x\n",
    "\n",
    "        def get_pd_pw(d, i, x_trials):\n",
    "            n_data, n_dimensions = d.shape\n",
    "            if n_dimensions==1:\n",
    "                i=1\n",
    "            t = d[:,i]\n",
    "            min_x = np.min(t)\n",
    "            max_x = np.max(t)\n",
    "            n_trials = x_trials.shape[0]\n",
    "            std_t = np.std(t)\n",
    "            if std_t==0:\n",
    "                h=0.005\n",
    "            else:\n",
    "                h=(4./(3*n_data))**(0.2)*std_t\n",
    "            prob_x = np.zeros((n_trials))\n",
    "            for j in range(n_trials):\n",
    "                prob_x[j] = get_prob_pw(x_trials[j],d,i,h)\n",
    "            return prob_x, x_trials, h\n",
    "\n",
    "        y_classes = np.unique(y_labels)\n",
    "        n_classes = len(y_classes)\n",
    "        n_trials = len(y_labels)\n",
    "        prob_w = []\n",
    "        x_cls = {}\n",
    "        for i in range(n_classes):\n",
    "            cls = y_classes[i]\n",
    "            cls_indx = np.where(y_labels == cls)[0]\n",
    "            prob_w.append(len(cls_indx) / n_trials)\n",
    "            x_cls.update({i: x_features[cls_indx, :]})\n",
    "\n",
    "        prob_x_w = np.zeros((n_classes, n_trials, x_features.shape[1]))\n",
    "        prob_w_x = np.zeros((n_classes, n_trials, x_features.shape[1]))\n",
    "        h_w_x = np.zeros((x_features.shape[1]))\n",
    "        mutual_info = np.zeros((x_features.shape[1]))\n",
    "        parz_win_width = 1.0 / np.log2(n_trials)\n",
    "        h_w = -np.sum(prob_w * np.log2(prob_w))\n",
    "\n",
    "        for i in range(x_features.shape[1]):\n",
    "            h_w_x[i] = 0\n",
    "            for j in range(n_classes):\n",
    "                prob_x_w[j, :, i] = get_pd_pw(x_cls.get(j), i, x_features[:, i])[0]\n",
    "\n",
    "        t_s = prob_x_w.shape\n",
    "        n_prob_w_x = np.zeros((n_classes, t_s[1], t_s[2]))\n",
    "        for i in range(n_classes):\n",
    "            n_prob_w_x[i, :, :] = prob_x_w[i] * prob_w[i]\n",
    "        prob_x = np.sum(n_prob_w_x, axis=0)\n",
    "        # prob_w_x = np.zeros((n_classes, prob_x.shape[0], prob_w.shape[1]))\n",
    "        for i in range(n_classes):\n",
    "            prob_w_x[i, :, :] = n_prob_w_x[i, :, :]/prob_x\n",
    "\n",
    "        for i in range(x_features.shape[1]):\n",
    "            for j in range(n_trials):\n",
    "                t_sum = 0.0\n",
    "                for k in range(n_classes):\n",
    "                    if prob_w_x[k, j, i] > 0:\n",
    "                        t_sum += (prob_w_x[k, j, i] * np.log2(prob_w_x[k, j, i]))\n",
    "\n",
    "                h_w_x[i] -= (t_sum / n_trials)\n",
    "\n",
    "            mutual_info[i] = h_w - h_w_x[i]\n",
    "\n",
    "        mifsg = np.asarray(mutual_info)\n",
    "        return mifsg\n",
    "\n",
    "\n",
    "    def select_CSP_pairs(self,features_selected,n_pairs):\n",
    "        features_selected+=1\n",
    "        sel_groups = np.unique(np.ceil(features_selected/n_pairs))\n",
    "        paired_features = []\n",
    "        for i in range(len(sel_groups)):\n",
    "            for j in range(n_pairs-1,-1,-1):\n",
    "                paired_features.append(sel_groups[i]*n_pairs-j)\n",
    "\n",
    "        paired_features = np.asarray(paired_features,dtype=np.int)-1\n",
    "\n",
    "        return paired_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_selection(x_features, y, n_features_select):\n",
    "        selector = FeatureSelect(n_features_select)\n",
    "        selector.fit(x_features, y)\n",
    "        return selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_score(signal, labels, cv_N = 5, classifier = False):\n",
    "    \n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    \n",
    "    if classifier:\n",
    "        y_pred = classifier.predict(signal)\n",
    "        acc = sklearn.metrics.accuracy_score(labels, y_pred)\n",
    "        return acc\n",
    "    \n",
    "    else:\n",
    "        # Assemble a classifier\n",
    "        svm = sklearn.svm.SVC()\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "#         lda = sklearn.ensemble.RandomForestClassifier()\n",
    "        csp = mne.decoding.CSP(n_components=99, reg=None, log=False, norm_trace=True)\n",
    "        # Use scikit-learn Pipeline with cross_val_score function\n",
    "        clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "#         clf = Pipeline([('CSP', csp), ('SVM', svm)])\n",
    "        scores = cross_val_score(clf, signal, labels, cv=cv_N, n_jobs=1)\n",
    "        _ = clf.fit(signal, labels)\n",
    "        return np.mean(scores), clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb309d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbcsp_score(signal, labels, m_filters, fs, cv_N = 5, classifier = [], n_select = 50):\n",
    "    # Create 4D matrix\n",
    "    filter_range = [4,40]\n",
    "    step = 4\n",
    "    filters_start = range(filter_range[0], filter_range[1], step)\n",
    "    filtered_data_4d = []\n",
    "    \n",
    "\n",
    "    for start_freq in filters_start:\n",
    "        filtered = mne.filter.filter_data(np.float64(signal), fs, start_freq, start_freq+step, method='fir', verbose=False)\n",
    "        filtered_data_4d.append(filtered)\n",
    "    filtered_data_4d = np.stack(filtered_data_4d, axis=0)\n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "    if classifier:\n",
    "        features = classifier[0].transform(filtered_data_4d) # fbcsp\n",
    "        features = classifier[1].transform(features) # feature selection\n",
    "        y_pred = classifier[2].predict(features) # classifier\n",
    "        acc = sklearn.metrics.accuracy_score(labels, y_pred)\n",
    "        return acc\n",
    "    else:\n",
    "        # implement CV\n",
    "        n_trials = labels.shape[0]\n",
    "        idx_perm = np.random.permutation(n_trials)\n",
    "        scores = []\n",
    "        # CV loop\n",
    "        for i in range(cv_N):\n",
    "            fractions = np.array_split(idx_perm, cv_N)\n",
    "\n",
    "            val_idx = list(fractions.pop(i))\n",
    "            train_idx = list(np.concatenate(fractions))\n",
    "            \n",
    "            X_train = filtered_data_4d[:,train_idx,:,:]\n",
    "            y_train = labels[train_idx]\n",
    "            X_val = filtered_data_4d[:,val_idx,:,:]\n",
    "            y_val = labels[val_idx]\n",
    "            \n",
    "            # Assemble classifier\n",
    "            fbcsp = FBCSP(m_filters=m_filters)\n",
    "#             clf = LinearDiscriminantAnalysis()\n",
    "#             clf = lgb.LGBMClassifier()\n",
    "            clf = sklearn.svm.SVC(verbose=0)\n",
    "            \n",
    "            # Train classifier\n",
    "            _ = fbcsp.fit(X_train, y_train)\n",
    "            features_train = fbcsp.transform(X_train)\n",
    "            selector = fit_selection(features_train, y_train, n_select)\n",
    "            features_train = selector.transform(features_train)\n",
    "            clf.fit(features_train, y_train)             \n",
    "            \n",
    "            # Evaluate classifier\n",
    "            features_val = fbcsp.transform(X_val)\n",
    "            features_val = selector.transform(features_val)\n",
    "            y_hat = clf.predict(features_val)\n",
    "            scores.append(sklearn.metrics.accuracy_score(y_val, y_hat)) \n",
    "           \n",
    "        # Train on full data\n",
    "        fbcsp = FBCSP(m_filters=m_filters)\n",
    "#         clf = LinearDiscriminantAnalysis()\n",
    "#         clf = lgb.LGBMClassifier()\n",
    "        clf = sklearn.svm.SVC()\n",
    "        \n",
    "        fbcsp.fit(filtered_data_4d, labels)\n",
    "        features = fbcsp.transform(filtered_data_4d)\n",
    "        selector = fit_selection(features, labels, n_select)\n",
    "        features = selector.transform(features)\n",
    "        clf.fit(features, labels)            \n",
    "        \n",
    "    return np.mean(scores), [fbcsp, selector, clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, fs=None, maxasterix=None):\n",
    "    \"\"\" \n",
    "    Annotate barplot with p-values.\n",
    "\n",
    "    :param num1: number of left bar to put bracket over\n",
    "    :param num2: number of right bar to put bracket over\n",
    "    :param data: string to write or number for generating asterixes\n",
    "    :param center: centers of all bars (like plt.bar() input)\n",
    "    :param height: heights of all bars (like plt.bar() input)\n",
    "    :param yerr: yerrs of all bars (like plt.bar() input)\n",
    "    :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1)\n",
    "    :param barh: bar height in axes coordinates (0 to 1)\n",
    "    :param fs: font size\n",
    "    :param maxasterix: maximum number of asterixes to write (for very small p-values)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data) is str:\n",
    "        text = data\n",
    "    else:\n",
    "        # * is p < 0.05\n",
    "        # ** is p < 0.005\n",
    "        # *** is p < 0.0005\n",
    "        # etc.\n",
    "        text = ''\n",
    "        p = .05\n",
    "\n",
    "        while data < p:\n",
    "            text += '*'\n",
    "            p /= 10.\n",
    "\n",
    "            if maxasterix and len(text) == maxasterix:\n",
    "                break\n",
    "\n",
    "        if len(text) == 0:\n",
    "            text = 'n. s.'\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    ly += yerr[num1]\n",
    "    ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    dh *= (ax_y1 - ax_y0)\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(ly, ry) + dh\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh+0.05)\n",
    "\n",
    "    plt.plot(barx, bary, c='black')\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "    if fs is not None:\n",
    "        kwargs['fontsize'] = fs\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_day_clf(EEGdict, AE_model):\n",
    "    # Use day zero classifier for classifying the reconstructed eeg per day\n",
    "    \n",
    "    # get relevant data\n",
    "    signal_test_data = EEGDataSet_signal_by_day(EEGdict, [0, len(EEGdict)])\n",
    "    orig_signal, _, labels = signal_test_data.getAllItems()\n",
    "    rec_signal = AE_model(orig_signal).detach().numpy()\n",
    "    res_signal = orig_signal - rec_signal\n",
    "    \n",
    "    # change labels from 1hot to int\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "    \n",
    "    score_orig, _ = csp_score(np.float64(orig_signal.detach().numpy()), labels, cv_N = 5, classifier = False)\n",
    "    score_rec, _ = csp_score(np.float64(rec_signal), labels, cv_N = 5, classifier = False)\n",
    "    score_res, _ = csp_score(np.float64(res_signal), labels, cv_N = 5, classifier = False)\n",
    "    return score_orig, score_rec, score_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet_signal_by_day(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1]):\n",
    "        \n",
    "        # Concat dict      \n",
    "        X, y, days_y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_channels = self.X.shape[1]\n",
    "        self.y = y\n",
    "        self.days_y = days_y\n",
    "        self.days_labels_N = days_range[1] - days_range[0]\n",
    "        self.task_labels_N = y.shape[1]\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.y[index], self.days_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.y, self.days_y\n",
    "    \n",
    "        \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        days_y = []\n",
    "        for day, d in enumerate(EEGDict[days_range[0]:days_range[1]]):\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "            days_y.append(np.ones_like(d['labels']) * day)\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        days_y = np.concatenate(days_y)\n",
    "        #  one hot encode days labels\n",
    "        y_temp = np.zeros((days_y.size, days_y.max() + 1))\n",
    "        y_temp[np.arange(days_y.size), days_y] = 1\n",
    "        days_y = y_temp\n",
    "        # One hot encode task labels\n",
    "        y_temp = np.zeros((y.size, y.max() + 1))\n",
    "        y_temp[np.arange(y.size), y] = 1\n",
    "        y = y_temp\n",
    "        return X, y, days_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet_signal(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1]):\n",
    "        \n",
    "        # Concat dict      \n",
    "        X, y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_channels = self.X.shape[1]\n",
    "        self.y = y\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.y\n",
    "    \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        for d in EEGDict[days_range[0]:days_range[1]]:\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_trials(dictListStacked, amp_thresh, min_trials):\n",
    "    # Remove noisy trials using amplitude threshold\n",
    "    new_dict_list = []\n",
    "    for i, D in enumerate(dictListStacked):\n",
    "        max_amp = np.amax(np.amax(D['segmentedEEG'], 2), 1)\n",
    "        min_amp = np.amin(np.amin(D['segmentedEEG'], 2), 1)\n",
    "        max_tr = max_amp > amp_thresh\n",
    "        min_tr = min_amp < -amp_thresh\n",
    "        noisy_trials = [a or b for a, b in zip(max_tr, min_tr)]\n",
    "        D['segmentedEEG'] = np.delete(D['segmentedEEG'], noisy_trials,axis=0)\n",
    "        D['labels'] = np.delete(D['labels'], noisy_trials,axis=0)\n",
    "    #    # One hot the labels\n",
    "    #     D['labels'][D['labels']==4] = 3\n",
    "    #     D['labels'] = torch.as_tensor(D['labels']).to(torch.int64) - 1\n",
    "    #     D['labels'] = F.one_hot(D['labels'], 3)\n",
    "        if D['segmentedEEG'].shape[0] > min_trials:\n",
    "                new_dict_list.append(D)\n",
    "\n",
    "    return new_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegFilters(eegMat, fs, filterLim):\n",
    "    eegMatFiltered = mne.filter.filter_data(eegMat, fs, filterLim[0], filterLim[1], verbose=0)\n",
    "    return eegMatFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caafbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c4893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
